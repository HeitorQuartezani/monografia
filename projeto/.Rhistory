subtitle = "Verificação de Variância Constante (Homocedasticidade)",
x = "Valores Preditos (Fitted)",
y = "Resíduos") +
theme_bw()
ggsave("diagnostico_2_homocedasticidade.png", plot = p_homo, width = 8, height = 6)
library(car)     # Para o qqPlot
library(ggplot2)
# 1. Recriar o modelo (caso tenha limpado a memória)
modelo_anova <- aov(answer_correctness_gpt.4o ~ chunking_strategy * search_type * model * top_k,
data = dados_limpos)
# Extrair resíduos
residuos <- residuals(modelo_anova)
ajustados <- fitted(modelo_anova)
# ------------------------------------------------------------------------------
# GRÁFICO 1: NORMALIDADE (QQ-PLOT)
# ------------------------------------------------------------------------------
# Vai aparecer na aba 'Plots' do RStudio agora
print(">>> Gerando QQ-Plot na tela...")
qqPlot(residuos,
main = "Normal Probability Plot (QQ-Plot)",
ylab = "Resíduos Studentizados",
xlab = "Quantis Normais",
col.lines = "red",
id = FALSE)
# PAUSA PRA VOCÊ VER O GRÁFICO (Se estiver rodando tudo de uma vez)
Sys.sleep(2)
# ------------------------------------------------------------------------------
# GRÁFICO 2: HOMOCEDASTICIDADE (RESÍDUOS vs PREDITOS)
# ------------------------------------------------------------------------------
print(">>> Gerando Resíduos vs Preditos na tela...")
df_diag <- data.frame(Preditos = ajustados, Residuos = residuos)
p_homo <- ggplot(df_diag, aes(x = Preditos, y = Residuos)) +
geom_point(alpha = 0.5, color = "darkblue") +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Plot of Residuals vs Fitted Values",
subtitle = "Verificação de Variância Constante",
x = "Valores Preditos",
y = "Resíduos") +
theme_bw()
# O ggplot precisa de um print explícito para aparecer quando roda em script
print(p_homo)
# ------------------------------------------------------------------------------
# TESTE NUMÉRICO
# ------------------------------------------------------------------------------
print("--- Teste de Shapiro-Wilk ---")
print(shapiro.test(residuos))
# 1. Recriar o modelo (caso tenha limpado a memória)
modelo_anova <- aov(answer_correctness_gpt.4o ~ chunking_strategy * search_type * model * top_k,
data = dados_limpos)
# Extrair resíduos
residuos <- residuals(modelo_anova)
ajustados <- fitted(modelo_anova)
# ------------------------------------------------------------------------------
# GRÁFICO 1: NORMALIDADE (QQ-PLOT)
# ------------------------------------------------------------------------------
# Vai aparecer na aba 'Plots' do RStudio agora
print(">>> Gerando QQ-Plot na tela...")
qqPlot(residuos,
main = "Normal Probability Plot (QQ-Plot)",
ylab = "Resíduos Studentizados",
xlab = "Quantis Normais",
col.lines = "red",
id = FALSE)
# ------------------------------------------------------------------------------
# GRÁFICO 2: HOMOCEDASTICIDADE (RESÍDUOS vs PREDITOS)
# ------------------------------------------------------------------------------
print(">>> Gerando Resíduos vs Preditos na tela...")
df_diag <- data.frame(Preditos = ajustados, Residuos = residuos)
p_homo <- ggplot(df_diag, aes(x = Preditos, y = Residuos)) +
geom_point(alpha = 0.5, color = "darkblue") +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Plot of Residuals vs Fitted Values",
subtitle = "Verificação de Variância Constante",
x = "Valores Preditos",
y = "Resíduos") +
theme_bw()
# O ggplot precisa de um print explícito para aparecer quando roda em script
print(p_homo)
metricas
library(car)     # Para o qqPlot
library(ggplot2)
# 1. Recriar o modelo (caso tenha limpado a memória)
modelo_anova <- aov(faithfulness_gpt.4o ~ chunking_strategy * search_type * model * top_k,
data = dados_limpos)
# Extrair resíduos
residuos <- residuals(modelo_anova)
ajustados <- fitted(modelo_anova)
# ------------------------------------------------------------------------------
# GRÁFICO 1: NORMALIDADE (QQ-PLOT)
# ------------------------------------------------------------------------------
# Vai aparecer na aba 'Plots' do RStudio agora
print(">>> Gerando QQ-Plot na tela...")
qqPlot(residuos,
main = "Normal Probability Plot (QQ-Plot)",
ylab = "Resíduos Studentizados",
xlab = "Quantis Normais",
col.lines = "red",
id = FALSE)
# PAUSA PRA VOCÊ VER O GRÁFICO (Se estiver rodando tudo de uma vez)
Sys.sleep(2)
# ------------------------------------------------------------------------------
# GRÁFICO 2: HOMOCEDASTICIDADE (RESÍDUOS vs PREDITOS)
# ------------------------------------------------------------------------------
print(">>> Gerando Resíduos vs Preditos na tela...")
df_diag <- data.frame(Preditos = ajustados, Residuos = residuos)
p_homo <- ggplot(df_diag, aes(x = Preditos, y = Residuos)) +
geom_point(alpha = 0.5, color = "darkblue") +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Plot of Residuals vs Fitted Values",
subtitle = "Verificação de Variância Constante",
x = "Valores Preditos",
y = "Resíduos") +
theme_bw()
# O ggplot precisa de um print explícito para aparecer quando roda em script
print(p_homo)
# ------------------------------------------------------------------------------
# TESTE NUMÉRICO
# ------------------------------------------------------------------------------
print("--- Teste de Shapiro-Wilk ---")
print(shapiro.test(residuos))
library(car)
library(ggplot2)
print(">>> Preparando os dados para transformação...")
N <- nrow(dados_limpos)
dados_limpos$faithfulness_adj <- (dados_limpos$faithfulness_gpt.4o * (N - 1) + 0.5) / N
# 2. APLICAR O LOGIT
# Agora transformamos para a escala logarítmica (Log-odds)
# log(p / (1 - p))
dados_limpos$faithfulness_logit <- log(dados_limpos$faithfulness_adj / (1 - dados_limpos$faithfulness_adj))
print(">>> Transformação concluída. Recalculando ANOVA...")
# 3. RECRIAR O MODELO COM A NOVA VARIÁVEL (LOGIT)
modelo_logit <- aov(faithfulness_logit ~ chunking_strategy * search_type * model * top_k,
data = dados_limpos)
# Extrair novos resíduos
residuos_logit <- residuals(modelo_logit)
ajustados_logit <- fitted(modelo_logit)
# GRÁFICO 1: QQ-PLOT (Esperamos que os pontos se aproximem mais da linha)
print(">>> Gerando QQ-Plot (Modelo Transformado)...")
qqPlot(residuos_logit,
main = "QQ-Plot (Dados Transformados via Logit)",
ylab = "Resíduos Studentizados (Escala Logit)",
xlab = "Quantis Normais",
col.lines = "blue", # Mudei a cor pra diferenciar
id = FALSE)
Sys.sleep(2)
# GRÁFICO 2: HOMOCEDASTICIDADE
# Esperamos que aquela linha diagonal inferior SUMA ou diminua drasticamente
print(">>> Gerando Resíduos vs Preditos (Modelo Transformado)...")
df_diag_logit <- data.frame(Preditos = ajustados_logit, Residuos = residuos_logit)
p_homo_logit <- ggplot(df_diag_logit, aes(x = Preditos, y = Residuos)) +
geom_point(alpha = 0.5, color = "darkgreen") + # Cor verde pra dar sorte
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Fitted (Modelo Logit)",
subtitle = "Verificação pós-transformação",
x = "Valores Preditos (Escala Logit)",
y = "Resíduos") +
theme_bw()
print(p_homo_logit)
# ------------------------------------------------------------------------------
# VERIFICAÇÃO FINAL
# ------------------------------------------------------------------------------
print("--- Novo Teste de Shapiro-Wilk (Ainda deve rejeitar devido ao N, mas observe o W) ---")
metricas
fatores
source("C:/monografia/analise/01_config_limpeza.R", echo=TRUE)
# ==============================================================================
# PACOTES NECESSÁRIOS
# ==============================================================================
if(!require(betareg)) install.packages("betareg")
if(!require(emmeans)) install.packages("emmeans")
if(!require(car)) install.packages("car")
library(betareg)
library(emmeans)
library(car)
# ==============================================================================
# FUNÇÃO MÁGICA: Analisa uma métrica e cospe os resultados
# ==============================================================================
analisar_metrica_rag <- function(dados, nome_coluna_y) {
message(paste(">>> ANALISANDO:", nome_coluna_y, "<<<"))
# 1. PREPARAÇÃO (Transformação de Smithson & Verkuilen para 0 e 1 exatos)
y_raw <- dados[[nome_coluna_y]]
n <- length(y_raw)
# Transforma para o intervalo (0, 1) estrito
y_beta <- (y_raw * (n - 1) + 0.5) / n
# Adiciona ao dataframe temporário
dados$Y_ATUAL <- y_beta
# 2. MODELAGEM (Regressão Beta)
# Usamos link logit (padrão).
# Se der erro de convergência, tente link = "probit" ou "cloglog"
modelo <- betareg(Y_ATUAL ~ chunking_strategy * search_type * model * top_k,
data = dados,
link = "logit")
# 3. TABELA DE "ANOVA" (Analysis of Deviance)
# Isso substitui sua tabela de ANOVA clássica. Mostra quem tem efeito significativo (p < 0.05)
print("--- Tabela de Significância (Wald Test) ---")
anova_table <- car::Anova(modelo, type = "III")
print(anova_table)
# 4. MÉDIAS ESTIMADAS (EMMEANS) - O QUE VAI PRO GRÁFICO
# Isso converte os log-odds de volta para % (0 a 1) para você explicar no texto
# Exemplo: Qual a melhor estratégia de Chunking?
print("--- Médias Marginais por Chunking Strategy ---")
em_chunk <- emmeans(modelo, ~ chunking_strategy, type = "response")
print(em_chunk)
print("--- Médias Marginais por Model ---")
em_model <- emmeans(modelo, ~ model, type = "response")
print(em_model)
# Retorna o modelo se quiser explorar mais
return(list(modelo = modelo, anova = anova_table, emmeans_chunk = em_chunk))
}
# Lista das suas métricas
metricas <- c("faithfulness_gpt.4o", "context_recall_gpt.4o",
"context_precision_gpt.4o", "answer_relevancy_gpt.4o",
"answer_correctness_gpt.4o")
# Loop para rodar tudo de uma vez
resultados <- list()
for (metrica in metricas) {
resultados[[metrica]] <- analisar_metrica_rag(dados_limpos, metrica)
cat("\n======================================================\n")
}
View(resultados)
modelo_para_teste <- resultados[["answer_relevancy_gpt.4o"]]$modelo
# ==============================================================================
# DIAGNÓSTICO VISUAL (4 GRÁFICOS EM 1 TELA)
# ==============================================================================
par(mfrow = c(2, 2)) # Divide a tela em 4
plot(modelo_para_teste, which = 1:4, type = "pearson")
par(mfrow = c(1, 1)) # Volta ao normal
# ==============================================================================
# DIAGNÓSTICO VISUAL (4 GRÁFICOS EM 1 TELA)
# ==============================================================================
par(mfrow = c(2, 2)) # Divide a tela em 4
plot(modelo_para_teste, which = 1:4, type = "pearson")
par(mfrow = c(1, 1)) # Volta ao normal
summary(modelo_para_teste)$pseudo.r.squared
View(dados_limpos)
View(dados_limpos)
source("C:/monografia/analise/01_config_limpeza.R", echo=TRUE)
# 1. Instalar pacotes necessários
if(!require(ARTool)) install.packages("ARTool")
if(!require(emmeans)) install.packages("emmeans")
library(ARTool)
library(emmeans)
library(dplyr)
# Função para rodar a ART para cada métrica
analisar_art_rag <- function(dados, nome_coluna_y) {
message(paste("\n>>> RODANDO ART PARA:", nome_coluna_y, "<<<"))
# A ART exige que os dados estejam limpos de NaNs
# Seleciona apenas as colunas necessárias e remove linhas incompletas
df_model <- dados[, c(nome_coluna_y, "chunking_strategy", "search_type", "model", "top_k")]
df_model <- na.omit(df_model)
# Converter fatores (garantia)
df_model$chunking_strategy <- as.factor(df_model$chunking_strategy)
df_model$search_type       <- as.factor(df_model$search_type)
df_model$model             <- as.factor(df_model$model)
df_model$top_k             <- as.factor(df_model$top_k)
# Renomeia Y para facilitar a fórmula
colnames(df_model)[1] <- "Y"
# ----------------------------------------------------------------------------
# 1. APLICAR A TRANSFORMAÇÃO (ALIGNED RANK TRANSFORM)
# ----------------------------------------------------------------------------
# Isso cria os ranks alinhados para cada efeito possível (Principais + Interações)
# OBS: Pode demorar um pouco se o N for muito grande
m_art <- art(Y ~ chunking_strategy * search_type * model * top_k, data = df_model)
# ----------------------------------------------------------------------------
# 2. TABELA DE ANOVA (NÃO-PARAMÉTRICA)
# ----------------------------------------------------------------------------
# Aqui vemos o que é significativo.
# A interpretação é idêntica à ANOVA comum: Olhe o Pr(>F)
print("--- Tabela de Significância (ART ANOVA) ---")
anova_art <- anova(m_art)
print(anova_art)
# ----------------------------------------------------------------------------
# 3. POST-HOC (COMPARAÇÕES PAR-A-PAR)
# ----------------------------------------------------------------------------
# Só faz sentido rodar se houver significância na tabela acima.
# Mas para o TCC, vamos gerar as médias dos ranks para os efeitos principais.
# Exemplo: Comparar Search Type (se deu significativo ou marginal)
print("--- Contraste: Search Type ---")
# art.con é a função específica para contrastes em modelos ART
try({
contraste_search <- art.con(m_art, "search_type", adjust = "bonferroni")
print(contraste_search)
}, silent = TRUE)
print("--- Contraste: Chunking Strategy ---")
try({
contraste_chunk <- art.con(m_art, "chunking_strategy", adjust = "bonferroni")
print(contraste_chunk)
}, silent = TRUE)
return(m_art)
}
# ------------------------------------------------------------------------------
# LOOP PARA TODAS AS MÉTRICAS
# ------------------------------------------------------------------------------
metricas <- c("faithfulness_gpt.4o", "context_recall_gpt.4o",
"context_precision_gpt.4o", "answer_relevancy_gpt.4o",
"answer_correctness_gpt.4o")
resultados_art <- list()
for (metrica in metricas) {
resultados_art[[metrica]] <- analisar_art_rag(dados_limpos, metrica)
}
View(dados_limpos)
View(dados_limpos)
source("C:/monografia/analise/01_config_limpeza.R", echo=TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)
# ==============================================================================
# 1. ANÁLISE DESCRITIVA VISUAL (HISTOGRAMAS)
# ==============================================================================
# Transforma os dados para formato longo para facilitar o plot com facet_wrap
dados_long <- dados_limpos %>%
select(faithfulness_gpt.4o, context_recall_gpt.4o,
context_precision_gpt.4o, answer_relevancy_gpt.4o,
answer_correctness_gpt.4o) %>%
pivot_longer(cols = everything(), names_to = "Metrica", values_to = "Score") %>%
mutate(Metrica = recode(Metrica,
"faithfulness_gpt.4o" = "Faithfulness",
"context_recall_gpt.4o" = "Context Recall",
"context_precision_gpt.4o" = "Context Precision",
"answer_relevancy_gpt.4o" = "Answer Relevancy",
"answer_correctness_gpt.4o" = "Answer Correctness"))
View(dados_limpos)
# ==============================================================================
# 1. ANÁLISE DESCRITIVA VISUAL (HISTOGRAMAS)
# ==============================================================================
# Transforma os dados para formato longo para facilitar o plot com facet_wrap
dados_long <- dados_limpos %>%
select(faithfulness_gpt.4o, context_recall_gpt.4o,
context_precision_gpt.4o, answer_relevancy_gpt.4o,
answer_correctness_gpt.4o) %>%
pivot_longer(cols = everything(), names_to = "Metrica", values_to = "Score") %>%
mutate(Metrica = recode(Metrica,
"faithfulness_gpt.4o" = "Faithfulness",
"context_recall_gpt.4o" = "Context Recall",
"context_precision_gpt.4o" = "Context Precision",
"answer_relevancy_gpt.4o" = "Answer Relevancy",
"answer_correctness_gpt.4o" = "Answer Correctness"))
# ==============================================================================
# 1. ANÁLISE DESCRITIVA VISUAL (HISTOGRAMAS) - CORRIGIDO
# ==============================================================================
library(ggplot2)
library(dplyr)
library(tidyr)
# Transforma os dados para formato longo
dados_long <- dados_limpos %>%
select(faithfulness_gpt.4o, context_recall_gpt.4o,
context_precision_gpt.4o, answer_relevancy_gpt.4o,
answer_correctness_gpt.4o) %>%
pivot_longer(cols = everything(), names_to = "Metrica", values_to = "Score") %>%
# AQUI ESTAVA O ERRO: Adicionei 'dplyr::' antes do recode
mutate(Metrica = dplyr::recode(Metrica,
"faithfulness_gpt.4o" = "Faithfulness",
"context_recall_gpt.4o" = "Context Recall",
"context_precision_gpt.4o" = "Context Precision",
"answer_relevancy_gpt.4o" = "Answer Relevancy",
"answer_correctness_gpt.4o" = "Answer Correctness"))
# Plot dos Histogramas
p <- ggplot(dados_long, aes(x = Score)) +
geom_histogram(bins = 20, fill = "#4E79A7", color = "white", alpha = 0.8) +
facet_wrap(~Metrica, scales = "free_y", ncol = 3) +
labs(title = "Distribuição das Métricas de Avaliação",
x = "Score [0-1]", y = "Frequência") +
theme_minimal() +
theme(strip.text = element_text(face = "bold", size = 10))
# Salva e exibe
ggsave("distribuicao_metricas.png", plot = p, width = 10, height = 6, dpi = 300)
print(p)
# ==============================================================================
# 2. FUNÇÃO PARA GERAR TABELAS LATEX (Média ± DP)
# ==============================================================================
gerar_tabela_descritiva <- function(dados, fator_coluna, metricas_alvo, nome_fator) {
# Agrupa e calcula Média e Desvio Padrão
tabela <- dados %>%
group_by(across(all_of(fator_coluna))) %>%
summarise(across(all_of(metricas_alvo),
list(mean = ~mean(., na.rm = TRUE),
sd = ~sd(., na.rm = TRUE)),
.names = "{.col}_{.fn}"))
# Formata para "0.00 ± 0.00"
tabela_formatada <- tabela
for(metrica in metricas_alvo) {
mean_col <- paste0(metrica, "_mean")
sd_col   <- paste0(metrica, "_sd")
tabela_formatada[[metrica]] <- paste0(
sprintf("%.2f", tabela_formatada[[mean_col]]),
" $\\pm$ ",
sprintf("%.2f", tabela_formatada[[sd_col]])
)
}
# Seleciona apenas as colunas formatadas
tabela_final <- tabela_formatada %>%
select(all_of(fator_coluna), all_of(metricas_alvo))
# Renomeia colunas para ficar bonito no LaTeX
names(tabela_final) <- gsub("_gpt.4o", "", names(tabela_final))
names(tabela_final) <- gsub("_", " ", names(tabela_final))
names(tabela_final)[1] <- "Nível"
# Gera o código LaTeX
print(kable(tabela_final, format = "latex", booktabs = TRUE,
caption = paste("Desempenho Médio por", nome_fator),
align = c("l", rep("c", length(metricas_alvo)))) %>%
kable_styling(latex_options = c("hold_position", "scale_down")))
}
# Definição dos grupos de métricas
metricas_retrieval <- c("context_recall_gpt.4o", "context_precision_gpt.4o", "answer_correctness_gpt.4o")
metricas_generation <- c("faithfulness_gpt.4o", "answer_relevancy_gpt.4o", "answer_correctness_gpt.4o")
# --- Tabela 1: Chunking (Foco em Recuperação + Correctness) ---
cat("\n--- Tabela Chunking ---\n")
gerar_tabela_descritiva(dados_limpos, "chunking_strategy", metricas_retrieval, "Estratégia de Chunking")
# --- Tabela 2: Busca (Foco em Recuperação + Correctness) ---
cat("\n--- Tabela Busca ---\n")
gerar_tabela_descritiva(dados_limpos, "search_type", metricas_retrieval, "Tipo de Busca")
# --- Tabela 3: Top-K (Foco em Recuperação + Correctness) ---
cat("\n--- Tabela Top-K ---\n")
gerar_tabela_descritiva(dados_limpos, "top_k", metricas_retrieval, "Top-K Documentos")
# --- Tabela 4: Modelo (Foco em Geração + Correctness) ---
# Aqui o recall/precision não importa tanto, pois o LLM não busca, ele só lê.
cat("\n--- Tabela Modelo ---\n")
gerar_tabela_descritiva(dados_limpos, "model", metricas_generation, "Modelo LLM")
# 1. Carregar bibliotecas com segurança
if(!require(dplyr)) install.packages("dplyr")
if(!require(kableExtra)) install.packages("kableExtra")
library(dplyr)
library(kableExtra)
# 2. Função "Fabrica de Tabelas"
# Essa função calcula média e desvio, formata como "0.85 ± 0.12" e gera o LaTeX
gerar_tabela_latex <- function(dados, coluna_grupo, nome_grupo, metricas, nomes_metricas_display) {
tabela <- dados %>%
group_by(across(all_of(coluna_grupo))) %>%
summarise(across(all_of(metricas),
~ paste0(sprintf("%.2f", mean(., na.rm = TRUE)),
" $\\pm$ ",
sprintf("%.2f", sd(., na.rm = TRUE))),
.names = "{.col}"))
# Renomear as colunas para ficar bonito no PDF
colnames(tabela) <- c(nome_grupo, nomes_metricas_display)
# Gerar o código LaTeX
kbl(tabela, format = "latex", booktabs = TRUE, linesep = "", align = "c",
caption = paste("Média e Desvio-Padrão por", nome_grupo)) %>%
kable_styling(latex_options = c("hold_position", "scale_down")) %>%
column_spec(1, bold = TRUE) # Deixa a primeira coluna (o fator) em negrito
}
# Grupo A: Métricas de Recuperação (Para Chunking, Busca e Top-K)
# Justificativa: Não faz sentido avaliar Faithfulness se o problema for Busca.
metricas_retrieval <- c("context_recall_gpt.4o", "context_precision_gpt.4o", "answer_correctness_gpt.4o")
labels_retrieval   <- c("Context Recall", "Context Precision", "Answer Correctness")
# Grupo B: Métricas de Geração (Para o Modelo LLM)
# Justificativa: O Modelo impacta como ele escreve (Faithfulness/Relevancy), não como ele busca.
metricas_generation <- c("faithfulness_gpt.4o", "answer_relevancy_gpt.4o", "answer_correctness_gpt.4o")
labels_generation   <- c("Faithfulness", "Answer Relevancy", "Answer Correctness")
# Tabela 1: Chunking Strategy (Usa métricas de Recuperação)
cat("\n\n--- CÓDIGO LATEX: CHUNKING ---\n")
print(gerar_tabela_latex(dados_limpos, "chunking_strategy", "Estratégia de Chunking",
metricas_retrieval, labels_retrieval))
# Tabela 2: Search Type (Usa métricas de Recuperação)
cat("\n\n--- CÓDIGO LATEX: BUSCA ---\n")
print(gerar_tabela_latex(dados_limpos, "search_type", "Tipo de Busca",
metricas_retrieval, labels_retrieval))
# Tabela 3: Top-K (Usa métricas de Recuperação)
cat("\n\n--- CÓDIGO LATEX: TOP-K ---\n")
# Dica: Convertendo top_k para fator para garantir ordem correta (5, 10, 15, 20)
dados_limpos$top_k_fator <- factor(dados_limpos$top_k, levels = c("5", "10", "15", "20"))
print(gerar_tabela_latex(dados_limpos, "top_k_fator", "Documentos (K)",
metricas_retrieval, labels_retrieval))
# Tabela 4: Modelo LLM (Usa métricas de Geração)
cat("\n\n--- CÓDIGO LATEX: MODELO LLM ---\n")
print(gerar_tabela_latex(dados_limpos, "model", "Modelo Gerador",
metricas_generation, labels_generation))
source("C:/monografia/analise/01_config_limpeza.R", echo=TRUE)
library(dplyr)
library(knitr)
# ==============================================================================
# FUNÇÃO PARA VISUALIZAR NO CONSOLE (SEM LATEX)
# ==============================================================================
visualizar_tabela <- function(dados, coluna_grupo, nome_grupo, metricas, nomes_metricas_display) {
tabela <- dados %>%
group_by(across(all_of(coluna_grupo))) %>%
summarise(across(all_of(metricas),
# Formato: "Média (DP)"
~ paste0(sprintf("%.2f", mean(., na.rm = TRUE)),
" (", sprintf("%.2f", sd(., na.rm = TRUE)), ")"),
.names = "{.col}"))
# Renomear colunas para leitura fácil
colnames(tabela) <- c(nome_grupo, nomes_metricas_display)
# Imprime bonito no console usando kable (formato pipe/markdown)
print(kable(tabela, format = "pipe", align = "c",
caption = paste("DESEMPENHO: Média (Desvio Padrão) por", nome_grupo)))
cat("\n") # Pula linha
}
# ==============================================================================
# DEFINIÇÃO DOS GRUPOS DE MÉTRICAS
# ==============================================================================
metricas_retrieval <- c("context_recall_gpt.4o", "context_precision_gpt.4o", "answer_correctness_gpt.4o")
labels_retrieval   <- c("Recall", "Precision", "Correctness")
metricas_generation <- c("faithfulness_gpt.4o", "answer_relevancy_gpt.4o", "answer_correctness_gpt.4o")
labels_generation   <- c("Faithfulness", "Relevancy", "Correctness")
# 1. Chunking
visualizar_tabela(dados_limpos, "chunking_strategy", "Chunking", metricas_retrieval, labels_retrieval)
# 2. Busca
visualizar_tabela(dados_limpos, "search_type", "Busca", metricas_retrieval, labels_retrieval)
# 3. Top-K (Ordenando numérico para não ficar 10, 15, 20, 5)
dados_limpos$top_k_fator <- factor(dados_limpos$top_k, levels = c("5", "10", "15", "20"))
visualizar_tabela(dados_limpos, "top_k_fator", "Top-K", metricas_retrieval, labels_retrieval)
source("C:/monografia/analise/01_config_limpeza.R", echo=TRUE)
source("C:/monografia/analise/02_descritiva.R", echo=TRUE)
source("C:/monografia/analise/02_descritiva.R", echo=TRUE)
source("C:/monografia/analise/03_modelagem.R", echo=TRUE)
source("C:/monografia/analise/03_modelagem.R", echo=TRUE)
source("C:/monografia/analise/01_config_limpeza.R", echo=TRUE)
source("C:/monografia/analise/03_modelagem.R", echo=TRUE)
source("C:/monografia/analise/03_modelagem.R", echo=TRUE)
